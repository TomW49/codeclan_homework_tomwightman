---
title: "Weekend Homework Solution - Model Building"
output:
  html_document:
    toc: true
    toc_float: true
    number_sections: true
    df_print: paged
    css: ../../../styles.css
  pdf_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, fig.align = 'center', warning = FALSE, message = FALSE)
```

# MVP

We've looked at a few different ways in which we can build models this week, including how to prepare them properly. This weekend we'll build a multiple linear regression model on a dataset which will need some preparation. The data can be found in the data folder, along with a data dictionary.  

We want to investigate the avocado dataset, and, in particular, to model the `AveragePrice` of the avocados. Use the tools we've worked with this week in order to prepare your dataset and find appropriate predictors. Once you've built your model use the validation techniques discussed on Wednesday to evaluate it. Feel free to focus either on building an *explanatory* or a *predictive* model, or both if you are feeling energetic!

As part of the MVP we want you not to just run the code but also have a go at **intepreting the results** and write your thinking in comments in your script.

**Hints and tips**

* `region` may lead to many dummy variables. Think carefully about whether to include this variable or not (there is no one 'right' answer to this!)
* Think about whether each variable is *categorical* or *numerical*. If categorical, make sure that the variable is represented as a factor.
* We will not treat this data as a time series, so `Date` will not be needed in your models, but can you extract any useful features out of `Date` before you discard it?
* If you want to build a predictive model, consider using either `leaps` or `glmulti` to help with this.


# Manual Approach : Explanatory Model


## Data & Data Cleaning 


<br> 

Load libraries:

```{r, warning = FALSE, message = FALSE}
library(tidyverse)
library(GGally)
library(modelr)
library(janitor)
```

Load dataset and examine it:

```{r}
avocados <- clean_names(read_csv("data/avocado.csv"))
head(avocados)
```
<br>

Ok, we have 14 variables. Can already see that some of them are somewhat useless (`x1` for example). Not sure whether the `total_bags` variable is the sum of `small_bags`, `large_bags` and `x_large_bags` so I'll check that first.

<br> 

```{r}
# check to see if total_bags variable is just the sum of the other three
avocados %>%
  mutate(total_sum = small_bags + large_bags + x_large_bags) %>%
  select(total_bags, total_sum)
```
<br> 

Yep, the `total_bags` column is just a sum of the other three. So this is a another variable I can get rid of. I can also check the same for volume:

```{r}
# check to see if total_volume variable is just the sum of the other three
avocados %>%
  mutate(total_sum = x4046 + x4225 + x4770) %>%
  select(total_volume, total_sum)
```

<br>

Nope, these aren't the same, so we can keep all these in. 

<br>

Now let's check how many different levels of each categorical variable we have.  

<br> 

```{r}
avocados %>%
  distinct(region) %>%
  summarise(number_of_regions = n())
```

```{r}
avocados %>%
  distinct(date) %>%
  summarise(
    number_of_dates = n(),
    min_date = min(date),
    max_date = max(date)
  )
```

<br>

The `region` variable will lead to many categorical levels, but we can try leaving it in. We should also examine `date` and perhaps pull out from it whatever features we can. Including every single date would be too much, so we can extract the different parts of the date that might be useful. For example, we could try and split it into different quarters, or years. 

So, let's do this now. Remove the variables we don't need, change our categorical variables to factors, and extract parts of the date in case they are useful (and get rid of date). 

<br>

```{r}
library(lubridate)
trimmed_avocados <- avocados %>%
  mutate(
    quarter = as_factor(quarter(date)),
    year = as_factor(year),
    type = as_factor(type),
    region = as_factor(region)
  ) %>%
  select(-c(x1, date,total_bags))
```

<br>

Now we've done our cleaning, we can check for aliased variables (i.e. combinations of variables in which one or more of the variables can be calculated exactly from other variables):

<br>

```{r}
alias(average_price ~ ., data = trimmed_avocados )
```

Nice, we don't find any aliases. So we can keep going. 

<br>


## First variable

We need to decide on which variable we want to put in our model first. To do this, we should visualise it. Because we have so much data, `ggpairs()` might take a while to run, so we can split it up a  bit. 

<br> 

```{r, message = FALSE}
# let's start by plotting the volume variables
trimmed_avocados %>%
  select(average_price, total_volume, x4046, x4225, x4770) %>%
  ggpairs() + 
   theme_grey(base_size = 8) # font size of labels
```
<br> 
Hmm, these look highly correlated with one another in some instances. This is a sign that we won't have to include all of these in our model, so we could think about removing `x4225` and `x4770` from our dataset to give ourselves fewer variables.   

```{r}
trimmed_avocados <- trimmed_avocados %>%
  select(-x4225, -x4770)
```
<br>

In terms of variables that correlate well with `average_price`... well none of them do, that well. But that's life. Our `x046` variable is probably our first candidate. 

Next we can look at our volume variables.
<br> 

```{r, message = FALSE}
trimmed_avocados %>%
  select(average_price, small_bags, large_bags, x_large_bags) %>%
  ggpairs() + 
   theme_grey(base_size = 8) # font size of labels
```
<br>

Hmm, again... not that promising. Some of the variables are highly correlated with one another, but not much seems highly correlated with `average_price`. 

<br> 

We can look at some of our categorical variables next: 

<br> 
```{r, message = FALSE}
trimmed_avocados %>%
  select(average_price, type, year, quarter) %>%
  ggpairs() + 
   theme_grey(base_size = 8) # font size of labels
```
<br> 

This seems better! Our `type` variable seems to show variation in the boxplots. This might suggest that conventional avocados and organic ones have different prices (which again, makes sense).  

Finally, we can make a boxplot of our `region` variable. Because this has so many levels, it makes sense to plot it by itself so we can see it.  

<br>

```{r}
trimmed_avocados %>%
  ggplot(aes(x = region, y = average_price)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```
<br>

Ok, seems there is some variation in the boxplots between different `regions`, so that seems like it could be promising. 

<br>

Let's start by test competing models. We decided that `x4046`, `type`, and `region` seemed reasonable:

<br>

```{r, warning = FALSE, message = FALSE}
library(ggfortify)

# build the model 
model1a <- lm(average_price ~ x4046, data = trimmed_avocados)

# check the diagnostics
autoplot(model1a)

# check the summary output
summary(model1a)
```


```{r}
# build the model 
model1b <- lm(average_price ~ type, data = trimmed_avocados)

# check the diagnostics
autoplot(model1b)

# check the summary output
summary(model1b)
```


```{r}
# build the model 
model1c <- lm(average_price ~ region, data = trimmed_avocados)

# check the diagnostics
autoplot(model1c)

# check the summary output
summary(model1c)
```

<br>

`model1b` with `type` is best, so we'll keep that and re-run `ggpairs()` with the residuals (again omitting `region` because it's too big).

<br>

## Second variable

<br>

```{r, warning = FALSE, message = FALSE}
avocados_remaining_resid <- trimmed_avocados %>%
  add_residuals(model1b) %>%
  select(-c("average_price", "type", "region"))

ggpairs(avocados_remaining_resid) + 
  theme_grey(base_size = 8) # this bit just changes the axis label font size so we can see
```
<br> 

Again, this isn't showing any really high correlations between the residuals and any of our numeric variables. 
Looks like `x4046`, `year`, `quarter` could show something potentially (given the rubbish variables we have). 

<br>

```{r}
trimmed_avocados %>%
  add_residuals(model1b) %>%
  ggplot(aes(x = region, y = resid)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, hjust = 1, vjust = 0.5))
```

<br>

Looks like `region` are our next contenders to try. Let's do these now. 

<br>

```{r}
model2a <- lm(average_price ~ type + x4046, data = trimmed_avocados)
autoplot(model2a)
summary(model2a)
```

```{r}
model2b <- lm(average_price ~ type + year, data = trimmed_avocados)
autoplot(model2b)
summary(model2b)
```

```{r}
model2c <- lm(average_price ~ type + quarter, data = trimmed_avocados)
autoplot(model2c)
summary(model2c)
```

```{r}
model2d <- lm(average_price ~ type + region, data = trimmed_avocados)
autoplot(model2d)
summary(model2d)
```

<br>

So `model2d` with `type` and `region` comes out as better here. We have some `region` coefficients that are not significant at $0.05$ level, so let's run an `anova()` to test whether to include `region`

<br>

```{r}
# model1b is the model with average_price ~ type
# model2d is the model with average_price ~ type + region

# we want to compare the two
anova(model1b, model2d)
```

<br>

It seems `region` is significant overall, so we'll keep it in!

<br>


## Third variable

<br>

`Model2d` is our model with `average_price ~ type + region`, and it explains `0.5473` of the variance in average price. This isn't really very high, so we can think about adding a third predictor now. Again, we want to remove these variables from our data, and check the residuals.  

<br> 

```{r, message = FALSE}
avocados_remaining_resid <- trimmed_avocados %>%
  add_residuals(model2d) %>%
  select(-c("average_price", "type", "region"))

ggpairs(avocados_remaining_resid) + 
   theme_grey(base_size = 8) # font size of labels
```

<br>

The next contender variables look to be `x_large_bags`, `year` and `quarter`. Let's try them out.

<br>

```{r}
model3a <- lm(average_price ~ type + region + x_large_bags, data = trimmed_avocados)
autoplot(model3a)
summary(model3a)
```

```{r}
model3b <- lm(average_price ~ type + region + year, data = trimmed_avocados)
autoplot(model3b)
summary(model3b)
```

```{r}
model3c <- lm(average_price ~ type + region + quarter, data = trimmed_avocados)
autoplot(model3c)
summary(model3c)
```

So `model3c` with `type`, `region` and `quarter` wins out here.
Everything still looks reasonable with the diagnostics, perhaps some mild heteroscedasticity.

<br>

## Fourth variable

<br>

Remember with two predictors, our R^2 variable was up at `0.5473`. Now, with three predictors, we are at `0.5874`. Ok, that seems reasonable as an improvement. So let's see how much improvement we get by adding a fourth variable. Again, check the residuals to see which ones we should try add.   

<br> 

```{r}
avocados_remaining_resid <- trimmed_avocados %>%
  add_residuals(model3c) %>%
  select(-c("average_price", "type", "region", "quarter"))

ggpairs(avocados_remaining_resid) + 
   theme_grey(base_size = 8) # font size of labels
```

<br>

The contender variables here are `x_large_bags` and `year`, so let's try them out.

<br>

```{r}
model4a <- lm(average_price ~ type + region + quarter + x_large_bags, data = trimmed_avocados)
autoplot(model4a)
summary(model4a)
```

```{r}
model4b <- lm(average_price ~ type + region + quarter + year, data = trimmed_avocados)
autoplot(model4b)
summary(model4b)
```

Hmm, `model4b` with `type`, `region`, `quarter` and `year` wins here. And it has improved our model performance from `0.5874` (with three predictors) to `0.6213`. That's quite good. 

<br>

## Fifth variable

<br>

We are likely now pursuing variables with rather limited explanatory power, but let's check for one more main effect, and see how much predictive power it gives us. 

<br>

```{r}
avocados_remaining_resid <- trimmed_avocados %>%
  add_residuals(model4b) %>%
  select(-c("average_price", "type", "region", "quarter", "year"))

ggpairs(avocados_remaining_resid) + 
   theme_grey(base_size = 8) # font size of labels
```
<br>

It looks like `x_large_bags` is the remaining contender, let's check it out!

<br> 

```{r}
model5 <- lm(average_price ~ type + region + quarter + year + x_large_bags, data = trimmed_avocados)
autoplot(model5)
summary(model5)
```

Overall, we still have some heterscedasticity and deviations from normality in the residuals. In terms of our regression summary, it is a significant explanatory variable, and it is significant. But hmmm... with four predictors, our overall R^2 was `0.6213`, and now with five we've only reached `0.6214`. Given that there is no real increase in explanatory performance, even though it's significant, we might want to remove it. Let's do this now.  

It's also clear we aren't gaining anything by adding predictors. The final thing we can do is test for interactions. 

<br>



## Pair interaction

Let's now think about possible pair interactions: for four main effect variables (`type + region + quarter + year`), so we have six possible pair interactions. Let's test them out.

  * type:region  
  * type:quarter  
  * type:year  
  * region:quarter  
  * region:year  
  * quarter:year  

Let's test these now:

<br> 

```{r}
model5pa <- lm(average_price ~ type + region + quarter + year + type:region, data = trimmed_avocados)
summary(model5pa)
```

```{r}
model5pb <- lm(average_price ~ type + region + quarter + year + type:quarter, data = trimmed_avocados)
summary(model5pb)
```

```{r}
model5pc <- lm(average_price ~ type + region + quarter + year + type:year, data = trimmed_avocados)
summary(model5pc)
```

```{r}
model5pd <- lm(average_price ~ type + region + quarter + year + region:quarter, data = trimmed_avocados)
summary(model5pd)
```

```{r}
model5pe <- lm(average_price ~ type + region + quarter + year + region:year, data = trimmed_avocados)
summary(model5pe)
```

```{r}
model5pf <- lm(average_price ~ type + region + quarter + year + quarter:year, data = trimmed_avocados)
summary(model5pf)
```

<br>

So it looks like `model5pa` with the `type`, `region`, `quarter`, `year`, and `type:region` is the best, with a moderate gain in multiple-$r^2$ due to the interaction. However, we need to test for the significance of the interaction given the various $p$-values of the associated coefficients

```{r}
anova(model5, model5pa)
```

Neat, it looks like including the interaction is statistically justified. So we can keep it in. And our final model is:

<center>

`average_price ~ type + region + quarter + year + type:region`

</center>

<hr>


# Automated approach : leaps

If you wanted to do a predictive (automatic) model, you could follow the same process, using the following code:

```{r}
library(leaps)

regsubsets_forward <- regsubsets(average_price ~ ., 
                                 data = trimmed_avocados, 
                                 nvmax = 12,
                                 method = "forward")

plot(regsubsets_forward)
```
<br>
From the plot, it seems like the best performing model has `type`, `year`, `region` (although not all of them are included), and `quarter`, although again, not all of them are included here. 

We can then plot the BIC score:

<br>

```{r}
# See what's in model
plot(summary(regsubsets_forward)$bic, type = "b")

```
From this, it seems like the BIC score doesn't really get that much lower after including 8 different variables. We can check which variables these are:

<br> 

```{r}
summary(regsubsets_forward)$which[8, ]
```
Given the ones that are true, best model includes `type`, `year`, some `regions` and some `quarters`. We can include type and year in our model, and then test whether quarter and region can be added.

```{r}
# test if we should put regions in
mod_type_year <- lm(average_price ~ type + year, data = trimmed_avocados)
mod_type_region <- lm(average_price ~ type + year + region, data = trimmed_avocados)
anova(mod_type_year, mod_type_region)

# yep, it's significant so we can put that in. 
```

```{r}
# test if we should put year in
mod_type_year <- lm(average_price ~ type + year, data = trimmed_avocados)
mod_type_quarter <- lm(average_price ~ type + year + quarter, data = trimmed_avocados)
anova(mod_type_year, mod_type_quarter)

# yep, it's significant so we can put that in. 

```

```{r}
# now let's test if the one with region and quarter is different than the one with just region

mod_type_region_quarter <- lm(average_price ~ type + year + region + quarter, data = trimmed_avocados)
anova(mod_type_region_quarter, mod_type_region)

# Yep, that's significant to I would leave it in. 
```




You can continue to test your interactions in the same way as we did during the manual version above. 






# Optional Extra: Automated approach, glmulti()

<br>

We didn't use this in class, but if you were interested in how to do model builing with `glmulti()`, you can run the code below.

<br>

<details>
<summary> **Automated approach : glmulti()** </summary>


```{r, eval = FALSE}
library(glmulti)
```

<br>

This data is pretty big for `glmulti` on a single CPU core, so we'll likely not be able to do a search simultaneously for both main effects and pairwise interactions. Let's look first for the best main effects model using BIC as our metric:

<br>

```{r, eval = FALSE}
# we're putting set.seed() in here for reproducibility, but you shouldn't include
# this in production code
set.seed(42)
n_data <- nrow(trimmed_avocados)
test_index <- sample(1:n_data, size = n_data * 0.2)

test  <- slice(trimmed_avocados, test_index)
train <- slice(trimmed_avocados, -test_index)

# sanity check
nrow(test) + nrow(train) == n_data
nrow(test)
nrow(train)
```

```{r, eval = FALSE}
glmulti_fit <- glmulti(
  average_price ~ ., 
  data = train,
  level = 1, # 2 = include pairwise interactions, 1 = main effects only (main effect = no pairwise interactions)
  minsize = 1, # no min size of model
  maxsize = -1, # -1 = no max size of model
  marginality = TRUE, # marginality here means the same as 'strongly hierarchical' interactions, i.e. include pairwise interactions only if both predictors present in the model as main effects.
  method = "h", # try exhaustive search, or could use "g" for genetic algorithm instead
  crit = bic, # criteria for model selection is BIC value (lower is better)
  plotty = FALSE, # don't plot models as function runs
  report = TRUE, # do produce reports as function runs
  confsetsize = 10, # return best 10 solutions
  fitfunction = lm # fit using the `lm` function
)
```

```{r, eval = FALSE}
summary(glmulti_fit)
```

<br>

So the lowest BIC model with main effects is `average_price ~ type + year + quarter + total_volume + x_large_bags + region`. Let's have a look at possible extensions to this. We're going to deliberately try to go to the point where models start to overfit (as tested by the RMSE on the test set), so we've seen what this looks like.

<br>

```{r, eval = FALSE}
results <- tibble(
  name = c(), bic = c(), rmse_train = c(), rmse_test = c()
)
```

<br>

```{r, eval = FALSE}
# lowest BIC model with main effects
lowest_bic_model <- lm(average_price ~ type + year + quarter + total_volume + x_large_bags + region, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "lowest bic", 
      bic = bic(lowest_bic_model),
      rmse_train = rmse(lowest_bic_model, train),
      rmse_test = rmse(lowest_bic_model, test)
    )
  )

# try adding in all possible pairs with these main effects
lowest_bic_model_all_pairs <- lm(average_price ~ (type + year + quarter + total_volume + x_large_bags + region)^2, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "lowest bic all pairs", 
      bic = bic(lowest_bic_model_all_pairs),
      rmse_train = rmse(lowest_bic_model_all_pairs, train),
      rmse_test = rmse(lowest_bic_model_all_pairs, test)
    )
  )

# try a model with all main effects
model_all_mains <- lm(average_price ~ ., data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "all mains", 
      bic = bic(model_all_mains),
      rmse_train = rmse(model_all_mains, train),
      rmse_test = rmse(model_all_mains, test)
    )
  )

# try a model with all main effects and all pairs
model_all_pairs <- lm(average_price ~ .^2, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "all pairs", 
      bic = bic(model_all_pairs),
      rmse_train = rmse(model_all_pairs, train),
      rmse_test = rmse(model_all_pairs, test)
    )
  )

# try a model with all main effects, all pairs and one triple (this is getting silly)
model_all_pairs_one_triple <- lm(average_price ~ .^2 + region:type:year, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "all pairs one triple",
      bic = bic(model_all_pairs_one_triple),
      rmse_train = rmse(model_all_pairs_one_triple, train),
      rmse_test = rmse(model_all_pairs_one_triple, test)
    )
  )

# try a model with all main effects, all pairs and multiple triples (more silly)
model_all_pairs_multi_triples <- lm(average_price ~ .^2 + region:type:year + region:type:quarter + region:year:quarter, data = train)
results <- results %>%
  add_row(
    tibble_row(
      name = "all pairs multi triples",
      bic = bic(model_all_pairs_multi_triples),
      rmse_train = rmse(model_all_pairs_multi_triples, train),
      rmse_test = rmse(model_all_pairs_multi_triples, test)
    )
  )
```

```{r, eval = FALSE}
results <- results %>%
  pivot_longer(cols = bic:rmse_test, names_to = "measure", values_to = "value") %>%
  mutate(
    name = fct_relevel(
      as_factor(name),
      "lowest bic", "all mains", "lowest bic all pairs", "all pairs", "all pairs one triple", "all pairs multi triples"
    )
  )
```

```{r, eval = FALSE}
results %>%
  filter(measure == "bic") %>%
  ggplot(aes(x = name, y = value)) +
  geom_col(fill = "steelblue", alpha = 0.7) +
  labs(
    x = "model",
    y = "bic"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  geom_hline(aes(yintercept = 0))
```

<br>

BIC is telling us here that if we took our main effects model with lowest BIC, and added in all possible pairs, this would likely still improve the model for predictive purposes. BIC suggests that this 'lowest BIC all pairs' model will offer best predictive performance without overfitting, with all other models being significantly poorer.

Let's compare the RMSE values of the various models for train and test sets. We expect train RMSE always to go down as model complexity increases, but what happens to the test RMSE as models get more complex?

<br>

```{r, eval = FALSE}
results %>%
  filter(measure != "bic") %>%
  ggplot(aes(x = name, y = value, fill = measure)) +
  geom_col(position = "dodge", alpha = 0.7) +
  labs(
    x = "model",
    y = "rmse"
  ) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```

<br>

Lowest RMSE in test is obtained for the 'lowest bic all pairs' model, and it increases thereafter for the more complex models, which suggests that these models are overfitting the training data.

</details> 
